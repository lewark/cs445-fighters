{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc8330c2-f84b-49ee-b5b3-287a2b1d367e",
   "metadata": {},
   "source": [
    "# Optimizing Reinforcement Algorithms & Comparing Performances with Street Fighter II: Special Champion Edition\n",
    "\n",
    "*by Josiah Hegarty, Everett Lewark, and Austin Youngren*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbbabb6-3b2f-44e4-b583-e99e7be00fc3",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68d126c-0e64-484c-bdf5-a177838e5bf0",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b09f91-0cc2-4762-8234-7d098f5996cb",
   "metadata": {},
   "source": [
    "### Designing Reward Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a247b42-c166-4e73-b7c2-7f4a4b5e69d1",
   "metadata": {},
   "source": [
    "During each time step, the game environment will return a dictionary containing information about the current game state. The particular variables exposed to the program vary by game. Within the default Street Fighter II environment, the following variables are exposed:\n",
    "\n",
    "- player score\n",
    "- player health\n",
    "- enemy health\n",
    "- player round wins\n",
    "- enemy round wins\n",
    "- the countdown timer\n",
    "\n",
    "Our initial reward function was fairly simple: reward the model when it does damage to the opponent, and penalize it when it takes damage. However, this simpler reward function has a problem. How, exactly, does a player deal damage? There are some ranged attacks in the game, but for the most part, dealing damage is more complex than just making a single action. In the case of melee attacks, a player must first walk toward the opponent before they can deal damage, and this is a whole task unto itself.\n",
    "\n",
    "Reinforcement learning models [tend to perform better](https://www.youtube.com/watch?v=IdJL9rcQrFU) when they are given some sort of continuous function they can try to optimize, but in the case of damage our rewards are much more sparse. The model would have to stumble into the other player and then happen to make a move that deals damage. To make matters worse, this would need to occur enough times that the model could learn a pattern from it, including both the ability to visually recognize where the player is relative to the enemy, as well the necessary actions to move it closer. This seems like a tall order.\n",
    "\n",
    "As a result, Everett tried introducing another variable into the reward function that rewarded the player for moving closer to the opponent. In order to do this, the X and Y coordinates of both the player and the enemy must be exposed from the environment so that they can be integrated into the reward function. The stable-retro library provides [documentation](https://stable-retro.farama.org/integration/) on the process of integrating a new game environment, and the process of modifying an existing integration is similar. The process involved compiling and running a specialized integration UI, which looked like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdabce67-c532-4cf9-b534-4fded3d9a702",
   "metadata": {},
   "source": [
    "![stable-retro integration UI](images/integration-ui.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0b03ca-2b7f-42f4-bcc8-1ecbc0be34d0",
   "metadata": {},
   "source": [
    "Using this interface, which functions similarly to other memory-inspection tools like Cheat Engine, Everett located variables within the game's RAM using an iterative process. For instance, to locate the player X coordinate, a search was performed for variables that were marked as unchanged. He then moved the player to the right and narrowed the current set of variables by searching for ones that increased. By following steps like these repeatedly, the console's entire RAM was gradually reduced to a few candidate memory locations, which were manually checked using the automatically-updating table in the sidebar. The same strategy was then used to locate the memory locations for the player Y coordinate and the enemy coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77bb8c4d-5eea-408d-be7d-ea25f1f7215c",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3ec545-4f4f-44da-96c9-003569ad0d28",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b6f795-27bf-4442-ad15-cd4564ac3134",
   "metadata": {},
   "source": [
    "# Word Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a6b5876-8680-4468-a57e-504615a089d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word count for file report.ipynb is 475\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "from nbformat import current\n",
    "import glob\n",
    "nbfile = glob.glob('report.ipynb')\n",
    "if len(nbfile) > 1:\n",
    "    print('More than one ipynb file. Using the first one.  nbfile=', nbfile)\n",
    "with io.open(nbfile[0], 'r', encoding='utf-8') as f:\n",
    "    nb = current.read(f, 'json')\n",
    "word_count = 0\n",
    "for cell in nb.worksheets[0].cells:\n",
    "    if cell.cell_type == \"markdown\":\n",
    "        word_count += len(cell['source'].replace('#', '').lstrip().split(' '))\n",
    "print('Word count for file', nbfile[0], 'is', word_count)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

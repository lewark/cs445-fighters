{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\xv}{\\mathbf{x}}\n",
    "\\newcommand{\\Xv}{\\mathbf{X}}\n",
    "\\newcommand{\\yv}{\\mathbf{y}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\Wv}{\\mathbf{W}}\n",
    "\\newcommand{\\wv}{\\mathbf{w}}\n",
    "\\newcommand{\\tv}{\\mathbf{t}}\n",
    "\\newcommand{\\Tv}{\\mathbf{T}}\n",
    "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Proposal for CS445"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Austin Youngren, Everett Lewark, Josiah Hegarty*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your project might involve some of the following:\n",
    "\n",
    "  * Apply a neural network learning algorithm and implementation covered in class applied to a data set of interest to you.  Analyze the results, in terms of training and testing accuracy, data samples for which the neural network output is incorrect, and displays of activities and/or weights in hidden units to help understand how the neural network is making its decisions.\n",
    "  * Download code from the net that implements an algorithm that was not covered in any of our assignments and apply it to data of interest to you.  Analyze the results, in terms of training and testing accuracy, data samples for which the neural network output is incorrect, and displays of activities and/or weights in hidden units to help understand how the neural network is making its decisions.\n",
    "  * Study at least five research articles from which you can learn about a machine learning topic of interest to you.  Your project will be a written report summarizing each article and describing similarities and differences between the papers, along with a detailed discussion of what you learned.\n",
    "  \n",
    "The above is not an exhaustive list.  This is your chance to be inventive and pursue some machine learning topic of your interest!  Project proposals will be reviewed to make sure they are appropriately scoped for the time period and team size, and to make sure you will put sufficient effort into both technical implementation and data analysis.\n",
    "\n",
    "  * What questions are you seeking to answer?\n",
    "  * What hypotheses can you make about the data you will be exploring?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning provides a framework allowing a machine learning model to explore some sort of action space. By rewarding actions that bring the model closer to a desired outcome, the model can be guided into creating its own solutions for environments such as playing games. \n",
    "\n",
    "#### Questions\n",
    "Popular algorithms for reinforcement learning include deep Q learning (DQN), advantage actor critic (A2C), and proximal policy optimization (PPO). We plan to experiment with these models and seek answers to the following questions: \n",
    "\n",
    "- Will one of these reinforcement learning models perform better than the others when training in 2D fighting game environments?   \n",
    "\n",
    "- Will the performance of these models vary across different 2D fighting games, or different characters within the same game, due to the variance of specific inputs needed to perform strings/combos? \n",
    "\n",
    "- In fighting games, players can choose what side of the screen to start on. Will this positioning affect the model, after being trained on one side, but having to play on the other? \n",
    "\n",
    "- (If we find we have more time) Will we see lower performance when we introduce models into a semi-3D environment due to the increase of actions the model can take?  \n",
    "\n",
    "- Do we see a different model perform better in this environment when compared to performance with 2D-fighters? \n",
    "\n",
    "#### Hypothesis\n",
    "With respect to model architectures, we expect the PPO2 model to have the best average performance on each game tested, followed by the A2C model, and with DQN having the worst average performance. Specifically, the PPO model will be able to win more sets of matches with less health lost in N sets than the other two models, when using the match wins and health percentages as an evaluation metric. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Explain why you wish to do the proposed project and the steps you will take to complete the project. Describe the methods you will use.  What are the sources of code or data?  Will you define new algorithms and/or implementations, or download ones from an on-line source?\n",
    "\n",
    "- You may work on your own, or form a team of up to four other students in this class. In this proposal, define how the work on the project will be divided among the team members.  It is not sufficient to just state that all team members will be working together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Motivation\n",
    "\n",
    "All three of us are interested in reinforcement learning, and this concept is a core reason we gained an interest in machine learning in the first place. In Everettâ€™s case, some of his first exposure was through YouTube videos by channels such as SethBling and suckerpinch. These videos applied [evolving-topology neural networks](https://www.youtube.com/watch?v=qv6UVOQ0F44) and [learned lexicographic orderings](https://www.youtube.com/watch?v=xOCurBYI_gY&pp=ygUQbGVhcm5mdW4gcGxheWZ1bg%3D%3D) to play Nintendo games.  \n",
    "\n",
    "We saw this term project as an opportunity to gain a higher level of knowledge on the concept. When we first met, Austin was hoping to train a model to play Tekken 8, but obtaining enough data to train a model to play the game would present a significant challenge. We also discussed the possibility of researching language models or implementing Gaussian Splatting to reconstruct real-world environments in 3D, but we ultimately decided to stick with reinforcement learning. By using an older game environment, we can take advantage of existing tools that allow access to internal game state. \n",
    "\n",
    "#### Tutorials and Libraries\n",
    "\n",
    "During this project, we plan to use some code from online tutorials for model training and evaluation. One of these, a [video tutorial](https://www.youtube.com/watch?v=rzbFhu6So5U) from Nicholas Renotte, covers the development of a model that plays Street Fighter. Typically, online tutorials use the [Gymnasium](https://gymnasium.farama.org/) and [stable-retro](https://stable-retro.farama.org/) libraries to integrate retro games into a reinforcement-learning environment. As is also discussed in [this Mortal Kombat project](https://medium.com/@zdwempe/using-reinforcement-learning-to-play-mortal-kombat-3-4f7e8bba7ab5) posted on Medium by Zach Wempe et al, these packages are often combined with the [stable-baselines3](https://stable-baselines3.readthedocs.io/en/master/) library. That library, an actively maintained version of an older library by OpenAI, provides implementations of common reinforcement learning models such as A2C and PPO. However, additional resources are provided within the PyTorch documentation, such as tutorials on [DQN](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html) and [PPO2](https://pytorch.org/tutorials/intermediate/reinforcement_ppo.html), as well as a tutorial on training a [Mario-playing ML agent](https://pytorch.org/tutorials/intermediate/mario_rl_tutorial.html). The latter two tutorials use a different library called [TorchRL](https://pytorch.org/rl/) for the models, and we may use this either in conjunction with or instead of stable-baselines3. \n",
    "\n",
    "#### Implementation\n",
    "\n",
    "Because information on the implementation of these models is somewhat challenging to find, and would be a complex process involving us studying the original [A3C](https://arxiv.org/abs/1602.01783) and [PPO](https://arxiv.org/pdf/1707.06347.pdf) papers, we decided it would be best to use the stable-baselines3 and/or TorchRL implementations. We would then focus on the analysis of fine-tuning model hyperparameters for model optimization, and compare model performance across multiple environments. The main hyperparameter we hope to focus on is the reward system by analyzing the importance of each reward in the learning process. We hope to take this a step further by creating a new reward system, made up of multiple reward pools, that will help with specifying to the model which aspects of the game it is doing well in, and where it needs to improve.  \n",
    "\n",
    "Using stable-baselines3 and/or TorchRL, we will train DQN, A2C, and PPO models, and the responsibility for each of these will be delegated to a particular team member. Josiah will focus on the DQN model, Everett will focus on the A2C model, and Austin will focus on the PPO model. After preliminary training, we will each research the possible hyperparameters that will aid in the optimization of the models. The hyperparameters may vary across models, so the points of research may vary from member to member.  \n",
    "\n",
    "When each of us have optimized functionality of our respective models, we will train each model on all three fighting games, Art of Fighting, Mortal Kombat III, and Street Fighter II-Special Champions. To improve the modelsâ€™ abilities to play these games, their progress will be rewarded based on the pre-defined reward systems that are specific to each game. Additionally, it may be possible for us to make further tweaks to the reward functions to identify how obscuring different information affects model performance. If time allows, we will also train the models on Virtua Fighter. Because this game has a semi-3D environment, it will present an added layer of complexity to the model.\n",
    "\n",
    "A possible additional goal is to allow people to play against the models trained against the trained models. The stable-retro library includes an example of [reading user input](https://github.com/Farama-Foundation/stable-retro/blob/master/retro/examples/retro_interactive.py). However, this may present challenges if the environments are not designed to accomodate 2-player mode, and may require the addition of custom savestates and action configurations.\n",
    "\n",
    "#### Games, Data Sources, and Reward Functions\n",
    "\n",
    "Our hope is to evaluate the performance of each type of reward within the rewards systems to see which of them have the highest impact on model performance improvement. We hope to plot each reward separately for this comparison.\n",
    "\n",
    "Reward Systems already implemented with Retro, found in json files of [GitHub - openai/retro: Retro Games in Gym](https://github.com/openai/retro/tree/master) and [GitHub: Farama-Foundation/stable-retro](https://github.com/Farama-Foundation/stable-retro/blob/master/retro/data): \n",
    "\n",
    "* <u>[Art of Fighting](https://www.emulatorgames.net/roms/super-nintendo/art-of-fighting/):</u> Enemy Health, Enemy Rounds Won, Health, Rounds Won, & Score \n",
    "\n",
    "* <u>[Mortal Kombat III](https://www.emulatorgames.net/roms/super-nintendo/ultimate-mortal-kombat-3/):</u> Enemy Health, Enemy Matches Won, Health, Matches Won, & Wins \n",
    "\n",
    "* <u>[Street Fighter II-Special Champions](https://www.emulatorgames.net/roms/sega-genesis/street-fighter-2-special-champion-edition/):</u> Continue Timer, Enemy Health, Enemy Matches Won, Health, Matches Won, & Score\t \n",
    "\n",
    "* <u>Reward systems common to all three 2D fighters:</u> Enemy health, enemy round/match wins, player round/matches wins, & health  \t \n",
    "\n",
    "* <u>[Virtua Fighter](https://archive.org/details/No-Intro-Collection_2016-01-03_Fixed):</u> Player health, enemy health, time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Possible Results\n",
    "\n",
    "Speculate on possible answers to the questions you provide in the Introduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible Results - Response\n",
    "\n",
    "We expect to see similar relative performance of the models on the games tested, with the PPO2 model having the best performance per game, followed by A2C, and then DQN with the worst performance per game. In addition, Virtua Fighter is expected to be the most challenging for our models to learn due to its semi-3D environment, which may introduce additional difficulty. We also expect to see a fair amount of variation in the specific results for each game since the specific scoring systems, time limit of each round, and other details will differ per game, although we expect to see similar general trends per game for each model.  \n",
    "\n",
    "There will also be variance in performance across all models through the various characters within each game: Inputs for certain characters may not be as intuitive for the model during training time. \n",
    "\n",
    "While not providing an inherent disadvantage (as in Tic-Tac-Toe), we expect the model will become confused in its ability to perform if it starts on the opposite side of the screen.  \n",
    "\n",
    "The model performance will worsen when 3D-fighting games are introduced due to the increase of actions that both the CPU and the model can make. This is due to an addition of having to react, or plan, to a new dimensional vertex of moves (a while standing jab could hit mid-range, while a side-step jab could hit high range). Additionally, fewer reward system items are available within Virtua Fighter with Retro as compared to the 2D-Fighting game counterparts (only player health, opponent health, and time). \n",
    "\n",
    "We expect to see the same model, PPO2, perform better than A3C or DQN within the 3D space environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timeline\n",
    "\n",
    "Make a list with at least four entries with dates and describe what each team member will accomplish by these dates.  This is for your use.  Your grade will not depend on meeting these deadlines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th style=\"text-align: center\">Dates</th>\n",
    "            <th colspan=\"3\" style=\"text-align: center\">Tasks</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td style=\"text-align: center\">4/6</td>\n",
    "            <td colspan=\"3\" style=\"text-align: center\">Proposal (As a Group)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: center\">4/7 -> 4/10</td>\n",
    "            <td colspan=\"3\" style=\"text-align: center\">Environment Setup (As a Group)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: center\">4/11 -> 4/13 </td>\n",
    "            <td style=\"text-align: center\">Set up PPO (Austin)</td>\n",
    "            <td style=\"text-align: center\">Set up A2C (Everett)</td>\n",
    "            <td style=\"text-align: center\">Set up DQN (Josiah)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: center\">4/14 -> 4/16</td>\n",
    "            <td style=\"text-align: center\">Train PPO model (Austin)</td>\n",
    "            <td style=\"text-align: center\">Train A2C model (Everett)</td>\n",
    "            <td style=\"text-align: center\">Train DQN model (Josiah)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: center\">4/17 -> 4/21</td>\n",
    "            <td style=\"text-align: center\">Tune PPO hyperparameters (Austin)</td>\n",
    "            <td style=\"text-align: center\">Tune A2C hyperparameters (Everett)</td>\n",
    "            <td style=\"text-align: center\">Tune DQN hyperparameters (Josiah)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: center\">4/22 -> 4/23</td>\n",
    "            <td colspan=\"3\" style=\"text-align: center\">Add user interface for player-vs-model (As a Group)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: center\">4/24 -> 5/6</td>\n",
    "            <td colspan=\"3\" style=\"text-align: center\">Collaboratively Integrate All Models as a Collective to Cross-Analyze Model Performances (As a Group)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: center\">4/24</td>\n",
    "            <td colspan=\"3\" style=\"text-align: center\">Ensure notebook ready for presentations (As a Group)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: center\">4/25</td>\n",
    "            <td colspan=\"3\" style=\"text-align: center\">Lightning Presentations Day 1</td>\n",
    "        </tr>        \n",
    "        <tr>\n",
    "            <td style=\"text-align: center\">5/2</td>\n",
    "            <td colspan=\"3\" style=\"text-align: center\">Lightning Presentations Day 2</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: center\">5/?</td>\n",
    "            <td colspan=\"3\" style=\"text-align: center\">Lightning Presentations Day 3</td>\n",
    "        </tr>\n",
    "            <tr>\n",
    "            <td style=\"text-align: center\">5/6 -> 5/9</td>\n",
    "            <td colspan=\"3\" style=\"text-align: center\">Ensure Notebook is Ready For Turn-In (As a Group)</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td style=\"text-align: center\">5/9</td>\n",
    "            <td colspan=\"3\" style=\"text-align: center\">Project Due Date @ 11:59PM</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

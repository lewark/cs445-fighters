{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7abcd248-4582-42f0-993d-ae89cc07f83e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gymnasium import Env\n",
    "from gymnasium.spaces import MultiBinary, Box\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import retro\n",
    "import time\n",
    "import torch\n",
    "\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1f18036-d2ec-4bb3-952d-29a4cc812e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(env):\n",
    "    obs, info = env.reset()\n",
    "    terminated = False\n",
    "    \n",
    "    for game in range(1):\n",
    "        while not terminated:\n",
    "            if terminated:\n",
    "                obs, info = env.reset()\n",
    "            env.render()\n",
    "            obs, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "            if reward != 0:\n",
    "                print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "473941ae-5c40-4cf9-9d6f-0c56f00fba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StreetFighter(Env):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.observation_space = Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "        self.action_space = MultiBinary(12)\n",
    "\n",
    "        self.game = retro.make(game='StreetFighterIISpecialChampionEdition-Genesis',\n",
    "                               use_restricted_actions=retro.Actions.FILTERED)\n",
    "        \n",
    "    def reset(self, seed=None):\n",
    "        obs, info = self.game.reset(seed=seed)\n",
    "        \n",
    "        obs = self.preprocess(obs)\n",
    "        self.previous_frame = obs\n",
    "\n",
    "        self.score = 0\n",
    "\n",
    "        return obs, info\n",
    "        \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.game.step(action)\n",
    "\n",
    "        obs = self.preprocess(obs)\n",
    "        \n",
    "        frame_delta = obs - self.previous_frame\n",
    "        self.previous_frame = obs\n",
    "\n",
    "        # plain score-based reward already implemented in default stable-retro config\n",
    "        # modify to include health\n",
    "        # score = info[\"score\"]\n",
    "        score = (\n",
    "            info[\"score\"]\n",
    "            + (info[\"health\"] - info[\"enemy_health\"])\n",
    "            + (info[\"matches_won\"] - info[\"enemy_matches_won\"]) * 1000\n",
    "        )\n",
    "        reward = score - self.score\n",
    "        self.score = score\n",
    "        \n",
    "        return frame_delta, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, *args, **kwargs):\n",
    "        self.game.render(*args, **kwargs)\n",
    "\n",
    "    def preprocess(self, img):\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (84, 84), interpolation=cv2.INTER_CUBIC)\n",
    "        channels = np.reshape(resize, (84, 84, 1))\n",
    "        return channels\n",
    "\n",
    "    def seed(self, seed):\n",
    "        pass\n",
    "    \n",
    "    def close(self):\n",
    "        self.game.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20a59741-ca78-4196-ad30-495ca08ef095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import optuna\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack, SubprocVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "269bce4d-9cc9-4e50-bbf8-e5d84a00d5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_agent(trial):\n",
    "    env = None\n",
    "    try:\n",
    "        n_cpus = 4\n",
    "        model_params = optimize_ppo(trial, n_cpus)\n",
    "\n",
    "        #env = StreetFighter()\n",
    "        # https://github.com/Farama-Foundation/stable-retro/blob/master/retro/examples/ppo.py\n",
    "        env = SubprocVecEnv([lambda: Monitor(StreetFighter(), LOG_DIR) for cpu in range(n_cpus)])\n",
    "        #env = Monitor(env, LOG_DIR)\n",
    "        #env = DummyVecEnv([lambda: env])\n",
    "        env = VecFrameStack(env, 4, channels_order='last')\n",
    "        \n",
    "        # https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html\n",
    "        device = torch.device('cuda:0')\n",
    "        model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=0, **model_params, device = device)\n",
    "        model.learn(total_timesteps=30000)\n",
    "        \n",
    "        mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=5)\n",
    "        env.close()\n",
    "        \n",
    "        SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(trial.number))\n",
    "        model.save(SAVE_PATH)\n",
    "        \n",
    "        return mean_reward\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return -1000\n",
    "    finally:\n",
    "        if env is not None:\n",
    "            env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5907b6f0-7447-4082-afc1-96d828b0ba99",
   "metadata": {},
   "source": [
    "[Parameter Policies](https://stable-baselines.readthedocs.io/en/master/modules/policies.html):\n",
    "\n",
    "* CnnPolicy: Policy object that implements actor critic, using a CNN (the nature CNN)\n",
    "* CnnLstmPolicy: Policy object that implements actor critic, using LSTMs with a CNN feature extraction\n",
    "* CnnLnLstmPolicy: Policy object that implements actor critic, using a layer normalized LSTMs with a CNN feature extraction\n",
    "* MlpPolicy: Policy object that implements actor critic, using a MLP (2 layers of 64)\n",
    "* MlpLstmPolicy: Policy object that implements actor critic, using LSTMs with a MLP feature extraction\n",
    "* MlpLnLstmPolicy: Policy object that implements actor critic, using a layer normalized LSTMs with a MLP feature extraction\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d9a5473-d5e4-4b2a-b721-938edb2e7b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = './logs/'\n",
    "OPT_DIR = './opt/'\n",
    "\n",
    "n_policy = ['CnnPolicy', 'MlpPolicy']\n",
    "n_learning_rate = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "batch_size = 64\n",
    "n_batches = [16, 32, 64, 128, 256]\n",
    "n_epochs = [50, 100, 500, 1000, 5000, 10000]\n",
    "n_procs = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428e77c0-b1cd-407d-9482-23cee560e698",
   "metadata": {},
   "source": [
    "## Hyperparameter Testing: Policy, Learning Rate, Batches, and Steps "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4574880-95aa-43c0-a3a6-272b78aad3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished architecture ['CnnPolicy', 0.1, 16, 50, -2000.0] at 524.5853402614594 seconds.\n",
      "finished architecture ['CnnPolicy', 0.1, 16, 100, -2000.0] at 645.3203458786011 seconds.\n"
     ]
    }
   ],
   "source": [
    "params = []\n",
    "for policy in n_policy:\n",
    "    for learning_rate in n_learning_rate:\n",
    "        for batches in n_batches:\n",
    "            for epochs in n_epochs:\n",
    "                start_time = time.time()\n",
    "                vec_env = make_vec_env(\"CartPole-v1\", n_envs=4)\n",
    "                env = SubprocVecEnv([lambda: Monitor(StreetFighter(), LOG_DIR) for proc in range(n_procs)])\n",
    "                env = VecFrameStack(env, 4, channels_order='last')\n",
    "                        # https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html\n",
    "                device = torch.device('cuda:0')\n",
    "                n_steps = batch_size * batches\n",
    "                model = PPO(policy = policy, env = env, learning_rate = learning_rate, n_steps = n_steps, batch_size = batch_size,\n",
    "                            n_epochs = epochs, tensorboard_log=LOG_DIR, verbose=0, device = device)\n",
    "                model.learn(total_timesteps=25000)\n",
    "        \n",
    "                mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=5)\n",
    "                env.close()\n",
    "    \n",
    "                hyper_ps = [policy, learning_rate, batches, epochs, mean_reward]\n",
    "                params.append(hyper_ps)\n",
    "                SAVE_PATH = os.path.join(OPT_DIR, 'trial_{}_best_model'.format(hyper_ps))\n",
    "                model.save(SAVE_PATH)\n",
    "                elapsed_time = time.time() - start_time\n",
    "                \n",
    "                print(f'finished architecture {hyper_ps} at {elapsed_time} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6013519a-21bb-408b-b6f2-1d3aaf470518",
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e249dd-74c8-4cf0-a15d-75b85e39330e",
   "metadata": {},
   "source": [
    "## Hyperparameter Testing: Gamma, Gae Lambda, and Clip Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fb11a0-e35e-4664-8ced-22d5d5e99c36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
